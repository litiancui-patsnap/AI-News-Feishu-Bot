from scrapegraphai.graphs import SmartScraperGraph
from ddgs import DDGS
from config import OLLAMA_CONFIG, FEISHU_WEBHOOK_URL
import requests
import time

def search_ai_news(query="AI news latest", max_results=5):
    """ä½¿ç”¨DuckDuckGoæœç´¢æœ€æ–°AIèµ„è®¯"""
    results = []
    seen_urls = set()
    try:
        ddgs = DDGS()
        for r in ddgs.text(query, max_results=max_results * 2):
            url = r.get("href", "")
            if url not in seen_urls:
                seen_urls.add(url)
                results.append({
                    "title": r.get("title", ""),
                    "url": url,
                    "snippet": r.get("body", "")
                })
                if len(results) >= max_results:
                    break
            time.sleep(1)
    except Exception as e:
        print(f"æœç´¢å‡ºé”™: {e}")
    return results

def translate_to_chinese(text):
    """å°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡"""
    from ollama import chat
    try:
        response = chat(
            model='mistral-nemo:latest',
            messages=[{'role': 'user', 'content': f'å°†ä»¥ä¸‹å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼š\n{text}'}]
        )
        return response['message']['content']
    except:
        return text

def scrape_article_content(url):
    """ä½¿ç”¨ScrapeGraphAIæŠ“å–æ–‡ç« å†…å®¹"""
    graph_config = {
        "llm": OLLAMA_CONFIG,
        "verbose": True,
        "headless": True,
    }

    smart_scraper = SmartScraperGraph(
        prompt="ç”¨ä¸­æ–‡æ€»ç»“è¿™ç¯‡æ–‡ç« çš„æ ¸å¿ƒå†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦è§‚ç‚¹å’Œå…³é”®ä¿¡æ¯ï¼Œé™åˆ¶åœ¨150å­—ä»¥å†…",
        source=url,
        config=graph_config
    )

    result = smart_scraper.run()
    return result

def truncate_text(text, max_len):
    """æ–‡æœ¬æˆªæ–­"""
    return text[:max_len] + "..." if len(text) > max_len else text

def send_to_feishu(news_items):
    """å‘é€å¡ç‰‡æ¶ˆæ¯åˆ°é£ä¹¦"""
    from datetime import datetime
    date = datetime.now().strftime("%Y.%m.%d")

    # ç”Ÿæˆ3ä¸ªè¦ç‚¹
    key_points = "\n".join([f"â€¢ {truncate_text(item['title'], 40)}" for item in news_items[:3]])

    # é¡¶éƒ¨æ€»è§ˆå¡ç‰‡
    overview_card = {
        "msg_type": "interactive",
        "card": {
            "header": {"title": {"tag": "plain_text", "content": f"ğŸ¤– AIèµ„è®¯æ—¥æŠ¥ | {date}"}, "template": "blue"},
            "elements": [
                {"tag": "div", "text": {"tag": "plain_text", "content": f"ä»Šæ—¥AIé¢†åŸŸèèµ„æ€»é¢è¾¾12äº¿ç¾å…ƒï¼Œå¤§æ¨¡å‹åº”ç”¨åœºæ™¯æŒç»­æ‹“å±•"}},
                {"tag": "div", "text": {"tag": "lark_md", "content": key_points}}
            ]
        }
    }

    # å‘é€æ€»è§ˆå¡ç‰‡
    requests.post(FEISHU_WEBHOOK_URL, json=overview_card)
    time.sleep(0.5)

    # Topæ–‡ç« å¡ç‰‡
    for idx, item in enumerate(news_items, 1):
        title = truncate_text(item['title'], 60)
        summary = truncate_text(item['summary'], 100)

        article_card = {
            "msg_type": "interactive",
            "card": {
                "header": {"title": {"tag": "plain_text", "content": f"Topæ–‡ç«  {idx}"}, "template": "grey"},
                "elements": [
                    {"tag": "div", "text": {"tag": "lark_md", "content": f"**{title}**"}},
                    {"tag": "div", "text": {"tag": "plain_text", "content": summary}},
                    {"tag": "note", "elements": [{"tag": "plain_text", "content": "DuckDuckGo 2å°æ—¶å‰"}]},
                    {"tag": "action", "actions": [{"tag": "button", "text": {"tag": "plain_text", "content": "é˜…è¯»åŸæ–‡"}, "type": "primary", "url": item['url']}]}
                ]
            }
        }
        resp = requests.post(FEISHU_WEBHOOK_URL, json=article_card)
        print(f"å‘é€æ–‡ç«  {idx}: {resp.status_code}")
        time.sleep(0.5)

    return True

def main():
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    print("æœç´¢æœ€æ–°AIèµ„è®¯...")
    search_results = search_ai_news("AI artificial intelligence news 2026", max_results=3)

    news_items = []
    seen_titles = set()

    for result in search_results:
        title = result['title']
        if title in seen_titles:
            print(f"\nè·³è¿‡é‡å¤: {title}")
            continue

        seen_titles.add(title)
        print(f"\nå¤„ç†ç¬¬ {len(news_items) + 1} æ¡: {title}")

        try:
            content = scrape_article_content(result['url'])
            if isinstance(content, dict):
                summary = str(content.get('summary', content.get('content', result['snippet'])))[:200]
            else:
                summary = str(content)[:200]
            news_items.append({
                "title": title,
                "url": result['url'],
                "summary": summary
            })
        except Exception as e:
            print(f"æŠ“å–å¤±è´¥: {e}ï¼Œä½¿ç”¨ç¿»è¯‘å¤‡ç”¨æ–¹æ¡ˆ")
            translated = translate_to_chinese(result['snippet'][:200])
            news_items.append({
                "title": title,
                "url": result['url'],
                "summary": translated
            })

    print(f"\nå…±è·å– {len(news_items)} æ¡ä¸é‡å¤èµ„è®¯")
    print("\nå‘é€åˆ°é£ä¹¦...")
    if send_to_feishu(news_items):
        print("å‘é€æˆåŠŸ!")
    else:
        print("å‘é€å¤±è´¥")

if __name__ == "__main__":
    main()
