from scrapegraphai.graphs import SmartScraperGraph
from ddgs import DDGS
from config import OLLAMA_CONFIG, FEISHU_WEBHOOK_URL, MAX_NEWS_ITEMS, SEARCH_QUERY
import requests
import time

def search_ai_news(query="AI news latest", max_results=5):
    """ä½¿ç”¨DuckDuckGoæœç´¢æœ€æ–°AIèµ„è®¯"""
    results = []
    seen_urls = set()
    try:
        ddgs = DDGS()
        for r in ddgs.text(query, max_results=max_results * 2):
            url = r.get("href", "")
            if url not in seen_urls:
                seen_urls.add(url)
                results.append({
                    "title": r.get("title", ""),
                    "url": url,
                    "snippet": r.get("body", "")
                })
                if len(results) >= max_results:
                    break
            time.sleep(1)
    except Exception as e:
        print(f"æœç´¢å‡ºé”™: {e}")
    return results

def translate_to_chinese(text):
    """å°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡"""
    from ollama import chat
    try:
        response = chat(
            model='mistral-nemo:latest',
            messages=[{'role': 'user', 'content': f'æå–ä»¥ä¸‹AIèµ„è®¯çš„æ ¸å¿ƒè§‚ç‚¹å’Œå…³é”®ä¿¡æ¯,ç”¨ä¸­æ–‡æ€»ç»“æˆ2-3å¥è¯,çªå‡ºæ–°é—»ä»·å€¼:\n{text}'}]
        )
        return response['message']['content']
    except:
        return text

def scrape_article_content(url):
    """ä½¿ç”¨ScrapeGraphAIæŠ“å–æ–‡ç« å†…å®¹"""
    graph_config = {
        "llm": OLLAMA_CONFIG,
        "verbose": True,
        "headless": True,
    }

    smart_scraper = SmartScraperGraph(
        prompt="ç”¨ä¸­æ–‡æ€»ç»“è¿™ç¯‡æ–‡ç« çš„æ ¸å¿ƒå†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦è§‚ç‚¹å’Œå…³é”®ä¿¡æ¯ï¼Œé™åˆ¶åœ¨150å­—ä»¥å†…",
        source=url,
        config=graph_config
    )

    result = smart_scraper.run()
    return result

def clean_title(title):
    """æ¸…ç†æ ‡é¢˜,ç§»é™¤ç½‘ç«™åç§°åç¼€"""
    import re
    # ç§»é™¤å¸¸è§çš„åˆ†éš”ç¬¦åŠå…¶åçš„å†…å®¹
    patterns = [r'\s*[|\-â€“â€”]\s*[A-Za-z\s]+$', r'\s*\|\s*.+$']
    for pattern in patterns:
        title = re.sub(pattern, '', title)
    return title.strip()

def extract_source(url):
    """ä»URLæå–æ¥æºç½‘ç«™åç§°"""
    from urllib.parse import urlparse
    domain = urlparse(url).netloc
    # ç§»é™¤www.å’Œå¸¸è§åç¼€
    domain = domain.replace('www.', '').split('.')[0]
    return domain.capitalize()

def get_topic_emoji(title, summary):
    """æ ¹æ®æ ‡é¢˜å’Œæ‘˜è¦æ¨æ–­ä¸»é¢˜emoji"""
    text = (title + ' ' + summary).lower()

    # æŒ‰ä¼˜å…ˆçº§åŒ¹é…å…³é”®è¯
    if any(word in text for word in ['èèµ„', 'æŠ•èµ„', 'æ”¶è´­', 'funding', 'investment', 'acquisition']):
        return 'ğŸ’°'
    elif any(word in text for word in ['å‘å¸ƒ', 'æ¨å‡º', 'launch', 'release', 'announce']):
        return 'ğŸš€'
    elif any(word in text for word in ['ç›‘ç®¡', 'æ³•è§„', 'æ”¿ç­–', 'regulation', 'policy', 'law']):
        return 'âš–ï¸'
    elif any(word in text for word in ['çªç ´', 'åˆ›æ–°', 'breakthrough', 'innovation']):
        return 'ğŸ”¬'
    elif any(word in text for word in ['æ¨¡å‹', 'gpt', 'llm', 'model', 'ai']):
        return 'ğŸ¤–'
    else:
        return 'ğŸ“°'

def truncate_text(text, max_len):
    """æ™ºèƒ½æˆªæ–­æ–‡æœ¬,åœ¨æ ‡ç‚¹ç¬¦å·å¤„æˆªæ–­"""
    if len(text) <= max_len:
        return text
    # åœ¨æ ‡ç‚¹ç¬¦å·å¤„æˆªæ–­
    truncated = text[:max_len]
    for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'ï¼Œ', ',']:
        last_punct = truncated.rfind(punct)
        if last_punct > max_len * 0.6:  # è‡³å°‘ä¿ç•™60%çš„å†…å®¹
            return truncated[:last_punct + 1]
    return truncated + "..."

def send_to_feishu(news_items):
    """å‘é€å¡ç‰‡æ¶ˆæ¯åˆ°é£ä¹¦"""
    from datetime import datetime
    date = datetime.now().strftime("%Y.%m.%d")

    # ç”Ÿæˆ3ä¸ªè¦ç‚¹
    key_points = "\n".join([f"â€¢ {truncate_text(clean_title(item['title']), 50)}" for item in news_items[:3]])

    # é¡¶éƒ¨æ€»è§ˆå¡ç‰‡
    overview_card = {
        "msg_type": "interactive",
        "card": {
            "header": {"title": {"tag": "plain_text", "content": f"ğŸ¤– AIèµ„è®¯æ—¥æŠ¥ | {date}"}, "template": "blue"},
            "elements": [
                {"tag": "div", "text": {"tag": "plain_text", "content": f"ä»Šæ—¥ç²¾é€‰ {len(news_items)} æ¡AIè¡Œä¸šé‡è¦èµ„è®¯"}},
                {"tag": "div", "text": {"tag": "lark_md", "content": key_points}}
            ]
        }
    }

    # å‘é€æ€»è§ˆå¡ç‰‡
    resp = requests.post(FEISHU_WEBHOOK_URL, json=overview_card)
    print(f"å‘é€æ€»è§ˆå¡ç‰‡: {resp.status_code}")
    time.sleep(0.5)

    # Topæ–‡ç« å¡ç‰‡
    for idx, item in enumerate(news_items, 1):
        title = truncate_text(clean_title(item['title']), 80)
        summary = truncate_text(item['summary'], 150)
        source = extract_source(item['url'])
        emoji = get_topic_emoji(item['title'], item['summary'])

        article_card = {
            "msg_type": "interactive",
            "card": {
                "header": {"title": {"tag": "plain_text", "content": f"{emoji} Topæ–‡ç«  {idx}"}, "template": "grey"},
                "elements": [
                    {"tag": "div", "text": {"tag": "lark_md", "content": f"**{title}**"}},
                    {"tag": "div", "text": {"tag": "plain_text", "content": summary}},
                    {"tag": "note", "elements": [{"tag": "plain_text", "content": f"æ¥æº: {source}"}]},
                    {"tag": "action", "actions": [{"tag": "button", "text": {"tag": "plain_text", "content": "é˜…è¯»åŸæ–‡"}, "type": "primary", "url": item['url']}]}
                ]
            }
        }
        resp = requests.post(FEISHU_WEBHOOK_URL, json=article_card)
        print(f"å‘é€æ–‡ç«  {idx}: {resp.status_code}")
        time.sleep(0.5)

    return True

def main():
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    print("æœç´¢æœ€æ–°AIèµ„è®¯...")
    search_results = search_ai_news(SEARCH_QUERY, max_results=MAX_NEWS_ITEMS)

    news_items = []
    seen_titles = set()

    for result in search_results:
        title = result['title']
        if title in seen_titles:
            print(f"\nè·³è¿‡é‡å¤: {title}")
            continue

        seen_titles.add(title)
        print(f"\nå¤„ç†ç¬¬ {len(news_items) + 1} æ¡: {title}")

        try:
            content = scrape_article_content(result['url'])
            if isinstance(content, dict):
                summary = str(content.get('summary', content.get('content', result['snippet'])))[:200]
            else:
                summary = str(content)[:200]
            news_items.append({
                "title": title,
                "url": result['url'],
                "summary": summary
            })
        except Exception as e:
            print(f"æŠ“å–å¤±è´¥: {e}ï¼Œä½¿ç”¨ç¿»è¯‘å¤‡ç”¨æ–¹æ¡ˆ")
            translated = translate_to_chinese(result['snippet'][:200])
            news_items.append({
                "title": title,
                "url": result['url'],
                "summary": translated
            })

    print(f"\nå…±è·å– {len(news_items)} æ¡ä¸é‡å¤èµ„è®¯")
    print("\nå‘é€åˆ°é£ä¹¦...")
    if send_to_feishu(news_items):
        print("å‘é€æˆåŠŸ!")
    else:
        print("å‘é€å¤±è´¥")

if __name__ == "__main__":
    main()
