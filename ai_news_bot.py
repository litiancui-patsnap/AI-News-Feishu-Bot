from scrapegraphai.graphs import SmartScraperGraph
from ddgs import DDGS
from config import OLLAMA_CONFIG, FEISHU_WEBHOOK_URL, MAX_NEWS_ITEMS, SEARCH_QUERY
import requests
import time
from datetime import datetime

def is_recent_article(url):
    """æ£€æŸ¥æ–‡ç« æ˜¯å¦ä¸ºæœ€è¿‘3å¤©å†…çš„"""
    import re
    # æå–URLä¸­çš„æ—¥æœŸæ¨¡å¼
    date_patterns = [
        r'/(\d{4})/(\d{2})/(\d{2})/',  # /2026/01/19/
        r'/(\d{4})-(\d{2})-(\d{2})',    # /2026-01-19
        r'(\d{4})(\d{2})(\d{2})',       # 20260119
    ]

    for pattern in date_patterns:
        match = re.search(pattern, url)
        if match:
            try:
                year, month, day = match.groups()
                article_date = datetime(int(year), int(month), int(day))
                days_old = (datetime.now() - article_date).days
                return days_old <= 3
            except:
                continue

    # å¦‚æœURLä¸­æ²¡æœ‰æ—¥æœŸï¼Œé»˜è®¤è®¤ä¸ºæ˜¯æœ€æ–°çš„
    return True

def search_ai_news(query="AI news latest", max_results=5):
    """ä½¿ç”¨DuckDuckGoæœç´¢æœ€æ–°AIèµ„è®¯"""
    results = []
    seen_urls = set()
    try:
        ddgs = DDGS()
        # å¢åŠ æœç´¢èŒƒå›´åˆ°5å€ï¼Œç¡®ä¿è¿‡æ»¤åä»æœ‰è¶³å¤Ÿç»“æœ
        for r in ddgs.text(query, max_results=max_results * 5):
            url = r.get("href", "")
            if url not in seen_urls and is_url_accessible(url) and is_recent_article(url):
                seen_urls.add(url)
                results.append({
                    "title": r.get("title", ""),
                    "url": url,
                    "snippet": r.get("body", "")
                })
                if len(results) >= max_results:
                    break
            time.sleep(0.5)
    except Exception as e:
        print(f"æœç´¢å‡ºé”™: {e}")
    return results

def is_url_accessible(url):
    """æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®ä¸”ä¸ºæœ‰æ•ˆæ–‡ç« """
    # è¿‡æ»¤åˆ†ç±»é¡µã€æ ‡ç­¾é¡µã€åˆ—è¡¨é¡µã€æ–°é—»æ±‡æ€»ç­‰éæ–‡ç« URL
    excluded_patterns = [
    '/category/', '/tag/', '/tags/', '/topics/', '/author/', '/page/',
    '/tagged/', '/news/', '/headlines/', '/ai-news', '/blog/', '/archive/',
    'roundup', 'weekly'
]

    if any(pattern in url.lower() for pattern in excluded_patterns):
        return False

    try:
        response = requests.head(url, timeout=5, allow_redirects=True)
        return response.status_code < 400
    except:
        return False

def translate_to_chinese(text):
    """å°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡"""
    from ollama import chat
    try:
        response = chat(
            model='mistral-nemo:latest',
            messages=[{'role': 'user', 'content': f'æå–ä»¥ä¸‹AIèµ„è®¯çš„æ ¸å¿ƒè§‚ç‚¹å’Œå…³é”®ä¿¡æ¯,ç”¨ä¸­æ–‡æ€»ç»“æˆ2-3å¥è¯,çªå‡ºæ–°é—»ä»·å€¼:\n{text}'}]
        )
        return response['message']['content']
    except Exception as e:
        print(f"ç¿»è¯‘å¤±è´¥: {e}")
        return text

def scrape_article_content(url):
    """ä½¿ç”¨ScrapeGraphAIæŠ“å–æ–‡ç« å†…å®¹"""
    graph_config = {
        "llm": OLLAMA_CONFIG,
        "verbose": True,
        "headless": True,
    }

    smart_scraper = SmartScraperGraph(
        prompt="""è¯·ç”¨ä¸­æ–‡ç”Ÿæˆä¸€æ®µ"è¯»å®Œå³å¤Ÿ"çš„AIèµ„è®¯æ‘˜è¦ï¼š
ä¼˜å…ˆæç‚¼æ–‡ç« çš„æ ¸å¿ƒç»“è®ºã€æœ€æ–°ä¿¡æ¯å’Œå®é™…å½±å“ï¼Œå¿½ç•¥èƒŒæ™¯é“ºå«ã€ä½œè€…ä»‹ç»ã€å¹¿å‘Šå’Œæ— å…³å†…å®¹ã€‚
ç”¨2â€“3å¥è¯å®Œæ•´è¡¨è¾¾"å‘ç”Ÿäº†ä»€ä¹ˆ + ä¸ºä»€ä¹ˆé‡è¦/å½±å“è°"ï¼Œå¦‚æœ‰æ˜ç¡®æ•°æ®ã€æ—¶é—´æˆ–ä¸»ä½“è¯·ä¿ç•™ã€‚
æ€»å­—æ•°ä¸è¶…è¿‡150å­—ã€‚""",
        source=url,
        config=graph_config
    )

    result = smart_scraper.run()
    return result

def clean_title(title):
    """æ¸…ç†æ ‡é¢˜,ç§»é™¤ç½‘ç«™åç§°åç¼€"""
    import re
    # ç§»é™¤å¸¸è§çš„åˆ†éš”ç¬¦åŠå…¶åçš„å†…å®¹
    patterns = [r'\s*[|\-â€“â€”]\s*[A-Za-z\s]+$', r'\s*\|\s*.+$']
    for pattern in patterns:
        title = re.sub(pattern, '', title)
    return title.strip()

def extract_source(url):
    """ä»URLæå–æ¥æºç½‘ç«™åç§°"""
    from urllib.parse import urlparse
    domain = urlparse(url).netloc
    # ç§»é™¤www.å’Œå¸¸è§åç¼€
    domain = domain.replace('www.', '').split('.')[0]
    return domain.capitalize()

def get_topic_emoji(title, summary):
    """æ ¹æ®æ ‡é¢˜å’Œæ‘˜è¦æ¨æ–­ä¸»é¢˜åˆ†ç±»æ ‡ç­¾"""
    text = (title + ' ' + summary).lower()

    if any(word in text for word in ['chip', 'gpu', 'nvidia', 'amd', 'èŠ¯ç‰‡', 'ç¡¬ä»¶', 'hardware']):
        return 'ğŸ–¥ï¸ èŠ¯ç‰‡/ç¡¬ä»¶'
    elif any(word in text for word in ['regulation', 'policy', 'law', 'ç›‘ç®¡', 'æ³•è§„', 'æ”¿ç­–', 'ä¼¦ç†']):
        return 'ğŸ“œ æ”¿ç­–/ä¼¦ç†'
    elif any(word in text for word in ['funding', 'investment', 'acquisition', 'company', 'èèµ„', 'æŠ•èµ„', 'æ”¶è´­', 'å…¬å¸', 'äº§ä¸š']):
        return 'ğŸ­ äº§ä¸š/å…¬å¸'
    elif any(word in text for word in ['weekly', 'brief', 'roundup', 'å‘¨æŠ¥', 'æ·±åº¦']):
        return 'ğŸ“Š å‘¨æŠ¥/æ·±åº¦'
    else:
        return 'ğŸ§  æ¨¡å‹/æŠ€æœ¯'

def is_encyclopedia_article(title, summary, url):
    """åˆ¤æ–­æ˜¯å¦ä¸ºç™¾ç§‘ç±»æ–‡ç« """
    text = (title + ' ' + summary).lower()
    return any(keyword in url.lower() for keyword in ['britannica', 'wikipedia', 'definition']) or \
           any(keyword in text for keyword in ['refers to', 'is defined as', 'æ˜¯æŒ‡', 'å®šä¹‰ä¸º'])

def generate_daily_insight(news_items):
    """ç”Ÿæˆä»Šæ—¥ä¸€å¥è¯åˆ¤æ–­"""
    from ollama import chat
    try:
        titles = '\n'.join([f"{i+1}. {item['title']}" for i, item in enumerate(news_items[:3])])
        response = chat(
            model='mistral-nemo:latest',
            messages=[{'role': 'user', 'content': f'åŸºäºä»¥ä¸‹3æ¡AIæ–°é—»æ ‡é¢˜ï¼Œç”¨ä¸€å¥è¯(20-30å­—)æ€»ç»“ä»Šæ—¥AIè¡Œä¸šçš„æ ¸å¿ƒè¶‹åŠ¿æˆ–è¦ç‚¹:\n{titles}\n\nè¦æ±‚ï¼šç®€æ´ã€æœ‰æ´å¯ŸåŠ›ã€çªå‡ºæœ€é‡è¦çš„ä¿¡å·'}]
        )
        insight = response['message']['content'].strip()
        return truncate_text(insight, 50)
    except:
        return "AIè¡Œä¸šæŒç»­å¿«é€Ÿå‘å±•ï¼Œå¤šä¸ªé¢†åŸŸå–å¾—é‡è¦è¿›å±•ã€‚"

def truncate_text(text, max_len):
    """æ™ºèƒ½æˆªæ–­æ–‡æœ¬,åœ¨æ ‡ç‚¹ç¬¦å·å¤„æˆªæ–­"""
    if len(text) <= max_len:
        return text
    # åœ¨æ ‡ç‚¹ç¬¦å·å¤„æˆªæ–­
    truncated = text[:max_len]
    for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'ï¼Œ', ',']:
        last_punct = truncated.rfind(punct)
        if last_punct > max_len * 0.6:  # è‡³å°‘ä¿ç•™60%çš„å†…å®¹
            return truncated[:last_punct + 1]
    return truncated + "..."

def send_to_feishu(news_items):
    """å‘é€å¡ç‰‡æ¶ˆæ¯åˆ°é£ä¹¦"""
    from datetime import datetime
    date = datetime.now().strftime("%Y.%m.%d")

    # åˆ†ç¦»ç™¾ç§‘ç±»æ–‡ç« å’Œæ­£å¸¸æ–‡ç« 
    encyclopedia_items = []
    main_items = []
    for item in news_items:
        if is_encyclopedia_article(item['title'], item['summary'], item['url']):
            encyclopedia_items.append(item)
        else:
            main_items.append(item)

    # ç”Ÿæˆä»Šæ—¥ä¸€å¥è¯åˆ¤æ–­
    daily_insight = generate_daily_insight(main_items[:3])

    # æ„å»ºæ–‡ç« åˆ—è¡¨å…ƒç´ 
    elements = [
        # é¦–å›¾
        {
            "tag": "img",
            "img_key": "https://raw.githubusercontent.com/litiancui-patsnap/AI-News-Feishu-Bot/main/images/ai_banner.png",
            "alt": {
                "tag": "plain_text",
                "content": "AIèµ„è®¯æ—¥æŠ¥"
            },
            "mode": "fit_horizontal",
            "preview": True
        },
        {"tag": "hr"},
        {"tag": "div", "text": {"tag": "plain_text", "content": f"ä»Šæ—¥ç²¾é€‰ {len(main_items)} æ¡AIè¡Œä¸šé‡è¦èµ„è®¯"}},
        {"tag": "div", "text": {"tag": "plain_text", "content": f"ğŸ§  ä»Šæ—¥AIè¦ç‚¹ï¼š{daily_insight}"}}
    ]

    # æ·»åŠ æ ‡é¢˜æ‘˜è¦åˆ—è¡¨
    title_list = []
    for item in main_items:
        title = truncate_text(clean_title(item['title']), 80)
        category = get_topic_emoji(item['title'], item['summary'])
        title_list.append(f"â€¢ {category} | {title}")

    elements.append({
        "tag": "div",
        "text": {"tag": "plain_text", "content": "\n".join(title_list)}
    })
    elements.append({"tag": "hr"})

    # æ·»åŠ æ¯ç¯‡æ–‡ç« 
    for idx, item in enumerate(main_items, 1):
        title = truncate_text(clean_title(item['title']), 80)
        summary = truncate_text(item['summary'], 150)
        source = extract_source(item['url'])
        category = get_topic_emoji(item['title'], item['summary'])

        # ç¬¬ä¸€æ¡åŠ ç„¦ç‚¹æ ‡è¯†
        if idx == 1:
            title_display = f"ğŸ”¥ ä»Šæ—¥ç„¦ç‚¹ï½œ{title}"
        else:
            title_display = title

        # æ–‡ç« æ ‡é¢˜å’Œæ‘˜è¦åˆå¹¶
        elements.append({
            "tag": "div",
            "text": {"tag": "lark_md", "content": f"**{category} | {title_display}**\n{summary}"}
        })
        # æ¥æºå’ŒæŒ‰é’®
        elements.append({
            "tag": "action",
            "actions": [
                {"tag": "button", "text": {"tag": "plain_text", "content": f"é˜…è¯»åŸæ–‡ Â· {source}"}, "type": "default", "url": item['url']}
            ]
        })
        # åˆ†éš”çº¿(æœ€åä¸€ç¯‡ä¸åŠ )
        if idx < len(main_items):
            elements.append({"tag": "hr"})

    # æ·»åŠ å»¶ä¼¸é˜…è¯»åŒº
    if encyclopedia_items:
        elements.append({"tag": "hr"})
        elements.append({
            "tag": "div",
            "text": {"tag": "plain_text", "content": "ğŸ“ å»¶ä¼¸é˜…è¯»"}
        })
        for item in encyclopedia_items:
            title = truncate_text(clean_title(item['title']), 60)
            source = extract_source(item['url'])
            elements.append({
                "tag": "div",
                "text": {"tag": "lark_md", "content": f"[{title}]({item['url']}) Â· {source}"}
            })

    # å•ä¸ªå¡ç‰‡åŒ…å«æ‰€æœ‰å†…å®¹
    card = {
        "msg_type": "interactive",
        "card": {
            "header": {"title": {"tag": "plain_text", "content": f"ğŸ¤– AIèµ„è®¯æ—¥æŠ¥ | {date}"}, "template": "blue"},
            "elements": elements
        }
    }

    resp = requests.post(FEISHU_WEBHOOK_URL, json=card)
    print(f"å‘é€å¡ç‰‡: {resp.status_code}")
    return resp.status_code == 200

def main():
    import sys
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

    print("æœç´¢æœ€æ–°AIèµ„è®¯...")
    search_results = search_ai_news(SEARCH_QUERY, max_results=MAX_NEWS_ITEMS)

    news_items = []
    seen_titles = set()

    for result in search_results:
        title = result['title']
        if title in seen_titles:
            print(f"\nè·³è¿‡é‡å¤: {title}")
            continue

        seen_titles.add(title)
        print(f"\nå¤„ç†ç¬¬ {len(news_items) + 1} æ¡: {title}")

        try:
            content = scrape_article_content(result['url'])
            if isinstance(content, dict):
                summary = str(content.get('summary', content.get('content', result['snippet'])))[:200]
            else:
                summary = str(content)[:200]
            news_items.append({
                "title": title,
                "url": result['url'],
                "summary": summary
            })
        except Exception as e:
            print(f"æŠ“å–å¤±è´¥: {e}ï¼Œä½¿ç”¨ç¿»è¯‘å¤‡ç”¨æ–¹æ¡ˆ")
            translated = translate_to_chinese(result['snippet'][:200])
            news_items.append({
                "title": title,
                "url": result['url'],
                "summary": translated
            })

    print(f"\nå…±è·å– {len(news_items)} æ¡ä¸é‡å¤èµ„è®¯")
    print("\nå‘é€åˆ°é£ä¹¦...")
    if send_to_feishu(news_items):
        print("å‘é€æˆåŠŸ!")
    else:
        print("å‘é€å¤±è´¥")

if __name__ == "__main__":
    main()
